{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e4eba57-dc59-4b8c-a9e1-86c6c33c30c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introduction to ML: Boston Housing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b449e1c-36cc-45f1-9d19-a7995cb62304",
   "metadata": {},
   "source": [
    "Lets download a well know ML dataset, the Boston Housing dataset to get some hands on experience with some basic yet crucial statistics. This is a [link](https://machinelearningmastery.com/standard-machine-learning-datasets/) to the dataset and below are some reference guides that people have used to analyze this dataset: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d909a2-dcc7-4e5d-accf-a38693f595ea",
   "metadata": {},
   "source": [
    "### EDA: Data Cleaning and Preliminary Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c01c89cb-c947-4968-a0c7-2883879a4917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "from numpy.random import randn\n",
    "from numpy import mean\n",
    "from numpy import var\n",
    "from numpy import std\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06a67f7-a3ed-48fb-ac7e-0b7cbca2a800",
   "metadata": {},
   "source": [
    "You'll notice when you open the raw data that there are 0 columns. We'll have to impute the values for the columns in our dataframe and read it in using the `pd.read_csv` function in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "729697d1-91ff-47ba-9781-4531d9b1e044",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make our column names so we can define them in our dataset\n",
    "columns = ['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE',\n",
    "           'DIS','RAD','TAX','PTRATIO','B','LSTAT','MEDV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9966574-d0bf-46e4-9a14-28e8325ecda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in our data and put in some extra things to make sure its all formatted correctly\n",
    "bh = pd.read_csv('housing.data', header=None, names=columns, delim_whitespace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9deb6c-a8a4-4f72-960c-504a35da10c0",
   "metadata": {},
   "source": [
    "**Meaning of Values in dataset:**\n",
    "\n",
    "- CRIM:  per capita crime rate by town.\n",
    "- ZN:    proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "- INDUS: proportion of nonretail business acres per town.\n",
    "- CHAS:  Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n",
    "- NOX:   nitric oxides concentration (parts per 10 million).\n",
    "- RM: average number of rooms per dwelling.\n",
    "- AGE: proportion of owner-occupied units built prior to 1940.\n",
    "- DIS: weighted distances to five Boston employment centers.\n",
    "- RAD: index of accessibility to radial highways.\n",
    "- TAX: full-value property-tax rate per 10,000 dollars.\n",
    "- PTRATIO: pupil-teacher ratio by town.\n",
    "- B: 1000(Bk – 0.63)^2 where Bk is the proportion of blacks by town.\n",
    "- LSTAT: % lower status of the population.\n",
    "- MEDV: Median value of owner-occupied homes in $1000s.\n",
    "\n",
    "**Prelim_Analysis**: Looking at the values in our data set, it looks like each observation is a essentially a town in boston and each variable is calculating porportions/averages of specific measures for each town. Our main dependent variable and target variable in this dataset will be 'MEDV' which will gives us a sense of house prices in that specific area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae8b2b64-ffbb-4a87-abda-d5a068201f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX   \n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296.0  \\\n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242.0   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242.0   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222.0   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets check to see if our dataset looks pretty.\n",
    "bh.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f63c4be-b9e5-4e17-9ddb-2804aa1b7fee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 14)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets take a look at our dataset as whole. No of rows/columns ect. We should have over 500 rows and 14 columns\n",
    "bh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d724d9-0200-4511-a5fe-2525ddb12c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the amount of null values we have. If we had null values we would want to convert them to 0s. We need all of our data to be in a numerical format for linear regression to work\n",
    "bh.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b693e5c0-7275-44f9-b422-a2cb116ac80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the cells that have null values\n",
    "bh[bh.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14582de-4976-4e52-8029-057f942034f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets check the our data types. We would expect everything to be numerical - integer or float\n",
    "bh.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d831ee-e645-494a-8946-741c10d77aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take a look at some general statistics\n",
    "bh.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07c1f6ff-27eb-45ea-96cf-48a29358fefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                   MEDV   R-squared:                       0.741\n",
      "Model:                            OLS   Adj. R-squared:                  0.734\n",
      "Method:                 Least Squares   F-statistic:                     108.1\n",
      "Date:                Thu, 25 May 2023   Prob (F-statistic):          6.72e-135\n",
      "Time:                        15:54:13   Log-Likelihood:                -1498.8\n",
      "No. Observations:                 506   AIC:                             3026.\n",
      "Df Residuals:                     492   BIC:                             3085.\n",
      "Df Model:                          13                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         36.4595      5.103      7.144      0.000      26.432      46.487\n",
      "CRIM          -0.1080      0.033     -3.287      0.001      -0.173      -0.043\n",
      "ZN             0.0464      0.014      3.382      0.001       0.019       0.073\n",
      "INDUS          0.0206      0.061      0.334      0.738      -0.100       0.141\n",
      "CHAS           2.6867      0.862      3.118      0.002       0.994       4.380\n",
      "NOX          -17.7666      3.820     -4.651      0.000     -25.272     -10.262\n",
      "RM             3.8099      0.418      9.116      0.000       2.989       4.631\n",
      "AGE            0.0007      0.013      0.052      0.958      -0.025       0.027\n",
      "DIS           -1.4756      0.199     -7.398      0.000      -1.867      -1.084\n",
      "RAD            0.3060      0.066      4.613      0.000       0.176       0.436\n",
      "TAX           -0.0123      0.004     -3.280      0.001      -0.020      -0.005\n",
      "PTRATIO       -0.9527      0.131     -7.283      0.000      -1.210      -0.696\n",
      "B              0.0093      0.003      3.467      0.001       0.004       0.015\n",
      "LSTAT         -0.5248      0.051    -10.347      0.000      -0.624      -0.425\n",
      "==============================================================================\n",
      "Omnibus:                      178.041   Durbin-Watson:                   1.078\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              783.126\n",
      "Skew:                           1.521   Prob(JB):                    8.84e-171\n",
      "Kurtosis:                       8.281   Cond. No.                     1.51e+04\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.51e+04. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "#defining our features and our target variable for OLS model\n",
    "X_af = bh.drop('MEDV', axis =1)\n",
    "y = bh['MEDV']\n",
    "X_af = sm.add_constant(X_af)\n",
    "\n",
    "# create formula for model 1\n",
    "model_1 = sm.OLS(y, X_af)\n",
    "result = model_1.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0834d1-71e6-48bf-9a98-aa39cc5130f9",
   "metadata": {},
   "source": [
    "### Data Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cebd180-4173-4ed2-9a8f-0db062c2aa5c",
   "metadata": {},
   "source": [
    "Lets take a look at our target variable to understand its distribution. We'll also plot our percentiles on the graph to visualize them as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a20a3fa-bdc3-4d06-8b43-9656f7db371f",
   "metadata": {},
   "source": [
    "#### Histogram for target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d24f30f-2ae8-42d2-b1db-3048ea8b8110",
   "metadata": {},
   "outputs": [],
   "source": [
    "p25 = np.percentile(bh['MEDV'], 25)\n",
    "p50 = np.percentile(bh['MEDV'], 50)\n",
    "p75 = np.percentile(bh['MEDV'], 75)\n",
    "\n",
    "sns.histplot(data=bh['MEDV'], kde=True, bins=30)\n",
    "\n",
    "plt.axvline(x=p25, color='red', linestyle='--', label='25th percentile')\n",
    "plt.axvline(x=p50, color='blue', linestyle='--', label='50th percentile')\n",
    "plt.axvline(x=p75, color='orange', linestyle='--', label='75th percentile')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06261b4-bfc5-4b68-a2cf-4256f56e8c7a",
   "metadata": {},
   "source": [
    "We can see that our data is slightly skewed to the right, meaning we have a positively skewed distribution. This is important to note for any transformations we decide to employ down the line. Generally, we don't like to have skewness in our data. For more on that refer to this [article](https://builtin.com/data-science/skewed-data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8d989c-cd61-4fbf-8fd4-6139592c39f2",
   "metadata": {},
   "source": [
    "#### Correlation Heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f82e03-6da8-451f-9682-66b9be774480",
   "metadata": {},
   "source": [
    "Now lets create a heatmap to view the correlation between MEDV and the other features in our data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a63f81-f00a-4cf0-856f-bc45ed3c9da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))         \n",
    "corr_matrix = bh.corr().round(2)\n",
    "sns.heatmap(data=corr_matrix, annot=True, cmap=sns.cubehelix_palette(as_cmap=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4b67be-02c9-4381-aeda-37bfe7506ab7",
   "metadata": {},
   "source": [
    "When looking at correlation heatmaps, correlation numbers higher than 0.7(absolute value) tends to mean there is a fairly strong positive correlation. Looking at our heatmap, the values that have the strongest relationship with MEDV are:\n",
    "- RM: average number of rooms per dwelling\n",
    "    - This has a positive relationship with MEDV, indicating that the more rooms there are, the higher the value the house tends to be. Which seems sound\n",
    "- LSTAT: % lower status of the population\n",
    "    - This has a negative relationship, indicating that the higher the porportion of lower status people, the higher the likelihood the prices would be lower. Also seems sound\n",
    "    \n",
    "These two features seem to have the most likely predictive strength for our MEDV target variable. Lets take a look at each of these variables' distrubutions and plot their values against MEDV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cf302e-00bf-44b9-8cb4-1f80f0d228d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets define our feautures \n",
    "features = ['LSTAT', 'RM']\n",
    "\n",
    "#create a good size for our visual \n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "#forloop to create a visual for each of our feautres\n",
    "for i, col in enumerate(features):\n",
    "    plt.subplot(1, len(features) , i+1)\n",
    "    p25 = np.percentile(bh[col], 25)\n",
    "    p50 = np.percentile(bh[col], 50)\n",
    "    p75 = np.percentile(bh[col], 75)\n",
    "\n",
    "    sns.histplot(data=bh[col], kde=True, bins=30)\n",
    "\n",
    "    plt.axvline(x=p25, color='red', linestyle='--', label='25th percentile')\n",
    "    plt.axvline(x=p50, color='blue', linestyle='--', label='50th percentile')\n",
    "    plt.axvline(x=p75, color='orange', linestyle='--', label='75th percentile')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b5501c-4614-4b1d-8245-289ebc5fc077",
   "metadata": {},
   "source": [
    "RM overall has a nice normal gaussian distribution\n",
    "\n",
    "The LSTAT(lower status) distribution clearly has a hard positive, right skew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3f3f38-972d-4bf4-93c6-0c3d1c6dadd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "\n",
    "features = ['LSTAT', 'RM']\n",
    "target = bh['MEDV']\n",
    "\n",
    "for i, col in enumerate(features):\n",
    "    plt.subplot(1, len(features), i+1)\n",
    "    x = bh[col]\n",
    "    y = target\n",
    "    plt.scatter(x,y, marker='o')\n",
    "    plt.title(col)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('MEDV')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31a6fbb-1e88-4a02-aafe-deab944234ab",
   "metadata": {},
   "source": [
    "We can clearly see that both variables have a fairly strong relationship with MEDV as expected. LSAT has a negative relationship while RM has a positive one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ace4c4-9391-4915-92d5-ed3f50b030d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preliminary Model Selection and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaa9075-4c55-4f1c-9908-cc0353c54063",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Preparing our Data for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "619b5af2-b7c4-4118-9fcb-bff0a89ae0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using numpy we're going to grab our features and put them into a data frame\n",
    "X = pd.DataFrame(np.c_[bh['LSTAT'], bh['RM'],bh['PTRATIO']], columns = ['LSTAT','RM','PTRATIO'])\n",
    "#We isolate our target variable\n",
    "Y = bh['MEDV']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdadaf7-b744-438f-b451-f05371024237",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Splitting ur data into the training set and the testing sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f7c958-040e-4043-b4ab-c711834e389e",
   "metadata": {},
   "source": [
    "Next, the data will have to be split into training sets, and testing sets. The model will be trained with about 80% of the data and we'll test 20% of the data on it. We have to do this to make sure our model performs well on unseen data. To ensure we have a good model that is trained well on our data. To split the data we use train_test_split function provided by scikit-learn library. Then we'll verify the size of our training and setting sets to ensure our data is distributed correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9708dd54-508a-4af6-825b-5a5f9a12ad64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 3)\n",
      "(102, 3)\n",
      "(404,)\n",
      "(102,)\n"
     ]
    }
   ],
   "source": [
    "#For our 2 feature model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state=5)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "982ae23d-b56d-47f7-aa91-cb83edf551e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>RM</th>\n",
       "      <th>PTRATIO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>18.35</td>\n",
       "      <td>5.701</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>3.16</td>\n",
       "      <td>7.923</td>\n",
       "      <td>13.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>20.62</td>\n",
       "      <td>5.957</td>\n",
       "      <td>20.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>9.08</td>\n",
       "      <td>6.120</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>20.31</td>\n",
       "      <td>6.404</td>\n",
       "      <td>20.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     LSTAT     RM  PTRATIO\n",
       "33   18.35  5.701     21.0\n",
       "283   3.16  7.923     13.6\n",
       "418  20.62  5.957     20.2\n",
       "502   9.08  6.120     21.0\n",
       "402  20.31  6.404     20.2"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17abc8ea-0073-4a9b-bf89-0b7d59d2295c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training the Model on training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eadbf1e-bed2-49b2-aae5-60fbf1dd1373",
   "metadata": {},
   "source": [
    "Things look good. Now lets use scikit-learn's `Linear-Regression` to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4e949c1-8958-41db-b27d-b455736e8380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "#Define our model\n",
    "l_model = LinearRegression()\n",
    "#Lets fit our Model to our features(X) and our target(Y) to the model\n",
    "l_model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f5fa363-b4ca-49a5-9de6-872dd63d9f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model performance for training set\n",
      "--------------------------------------\n",
      "RMSE is 5.303863193307931\n",
      "R2 score is 0.6725214335656511\n"
     ]
    }
   ],
   "source": [
    "#Fitting our linear model to our features(2) to see what they give us as a prediction for y\n",
    "Y_train_predict = l_model.predict(X_train)\n",
    "#Evaluating our predicted values against the actual values to get a sense of accuracy\n",
    "rmse = (np.sqrt(mean_squared_error(Y_train, Y_train_predict)))\n",
    "#Looking to see how much of the variation in our data our model explains\n",
    "r2 = r2_score(Y_train, Y_train_predict)\n",
    "\n",
    "print(\"model performance for training set\")\n",
    "print(\"--------------------------------------\")\n",
    "print('RMSE is {}'.format(rmse))\n",
    "print('R2 score is {}'.format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752dc19a-e230-497e-9927-d581dad7726d",
   "metadata": {},
   "source": [
    "This is a fairly good baseline to start of with. Our rmse is fairly okay and our R2 explains 67% of the variation in our dataset. Now lets see how our model will perform on data it hasn't seen before. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dc97aa-7e19-4db9-ad68-8e080b9716af",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Testing the model on our test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "659cb5cf-b26a-4ad2-9d9f-3b42b44a3352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model evaluation for test set\n",
      "-----------------------------\n",
      "RMSE is 4.9139375347640835\n",
      "R2 is 0.691587828074417\n"
     ]
    }
   ],
   "source": [
    "#Same tests for the test set\n",
    "Y_test_predict = l_model.predict(X_test)\n",
    "rmse = (np.sqrt(mean_squared_error(Y_test, Y_test_predict)))\n",
    "r2 = r2_score(Y_test, Y_test_predict)\n",
    "\n",
    "print('model evaluation for test set')\n",
    "print('-----------------------------')\n",
    "print('RMSE is {}'.format(rmse))\n",
    "print('R2 is {}'.format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0bb1f2-bf92-49cb-99dc-5a903ddad530",
   "metadata": {},
   "source": [
    "Our rmse is lower and he r2 is higher, all good signs. We have a good baseline to start of with but now we want to make our model even better, so we'll have to employ more methods to our dataset to get closer to a better model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b486762c-c398-4148-88e7-779646ded101",
   "metadata": {},
   "source": [
    "### Test this on our model with 13 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a911c95a-eca4-443b-8f1b-ea220d6b81c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining our features and our target variable for OLS model\n",
    "X_af = bh.drop('MEDV', axis =1)\n",
    "y = bh['MEDV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac55cad4-2bb6-463f-96be-d349bef39da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 13)\n",
      "(102, 13)\n",
      "(404,)\n",
      "(102,)\n"
     ]
    }
   ],
   "source": [
    "#Doing the same for all features\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_af, X_test_af, y_train, y_test = train_test_split(X_af, y, test_size = 0.2, random_state=5)\n",
    "print(X_train_af.shape)\n",
    "print(X_test_af.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b1e6a05b-fc5a-408c-a9a1-e6f87b812e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Define our model\n",
    "l_af_model = LinearRegression()\n",
    "#Lets fit our Model to our features(X) and our target(Y) to the model\n",
    "l_af_model.fit(X_train_af, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1c9eebc-f9d7-4f21-8ac7-eea38cb4fbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model performance for training set\n",
      "--------------------------------------\n",
      "RMSE is 4.741000992236517\n",
      "R2 score is 0.7383393920590519\n"
     ]
    }
   ],
   "source": [
    "#Fitting our linear model to our features(13) to see what they give us as a prediction for y\n",
    "y_train_predict = l_af_model.predict(X_train_af)\n",
    "#Evaluating our predicted values against the actual values to get a sense of accuracy\n",
    "rmse = (np.sqrt(mean_squared_error(y_train, y_train_predict)))\n",
    "#Looking to see how much of the variation in our data our model explains\n",
    "r2 = r2_score(y_train, y_train_predict)\n",
    "\n",
    "print(\"model performance for training set\")\n",
    "print(\"--------------------------------------\")\n",
    "print('RMSE is {}'.format(rmse))\n",
    "print('R2 score is {}'.format(r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f4dc994-139b-4ff0-9ccd-2d56b963fa3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model evaluation for test set\n",
      "-----------------------------\n",
      "RMSE is 4.5682920423032\n",
      "R2 is 0.7334492147453086\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = l_af_model.predict(X_test_af)\n",
    "rmse = (np.sqrt(mean_squared_error(y_test, y_test_pred)))\n",
    "r2 = r2_score(y_test, y_test_pred)\n",
    "print('model evaluation for test set')\n",
    "print('-----------------------------')\n",
    "print('RMSE is {}'.format(rmse))\n",
    "print('R2 is {}'.format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200e51b6-b100-45f8-9d17-63fab76262a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creating Better Models\n",
    "- Lets Discuss Assumptions of a linear model and whether our data completely fits those [assumptions](https://towardsdatascience.com/top-3-methods-for-handling-skewed-data-1334e0debf45)\n",
    "- Lets fit a [Polynomial model](https://medium.com/towards-data-science/polynomial-regression-bbe8b9d97491) to our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9594b7a9-0ca4-4bc6-a730-78fc18d36477",
   "metadata": {},
   "source": [
    "Linear Regression models make a great amount of *assumptions* about our dataset. These assumptions are:\n",
    "- 1. **Linearity**: assumes that the relationship between predictors and target variable is linear\n",
    "\n",
    "- 2. **No noise**: eg. that there are no outliers in the data\n",
    "\n",
    "- 3. **No collinearity**: if you have highly correlated predictors, it’s most likely your model will overfit\n",
    "\n",
    "- 4. **Normal distribution**: more reliable predictions are made if the predictors and the target variable are normally distributed\n",
    "\n",
    "- 5. **Scale**: it’s a distance-based algorithm, so preditors should be scaled — like with standard scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38be87d2-074d-4c7c-a3db-998b1d5e7dda",
   "metadata": {},
   "source": [
    "When the data does not completely fit these assumptions, some transformations are applied to our training data to give our variables a better relationship for our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9469d1e1-a86c-4d5e-9885-4e9f367aaf76",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Logarithimic Transformations \n",
    "Lets tackle the 4th assumption of Normal distributions. Lets look at all of our variables, including our feautres and target variable to look at their distributions to check to see how skewed they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a57c468-1b36-4849-899f-16b6d8137c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feautures = [X_train['RM'], X_train['LSTAT']]\n",
    "\n",
    "#create a good size for our visual \n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "#forloop to create a visual for each of our feautres\n",
    "for i, col in enumerate(features):\n",
    "    plt.subplot(1, len(features) , i+1)\n",
    "    p25 = np.percentile(X_train[col], 25)\n",
    "    p50 = np.percentile(X_train[col], 50)\n",
    "    p75 = np.percentile(X_train[col], 75)\n",
    "\n",
    "    sns.histplot(data=X_train[col], kde=True, bins=30)\n",
    "\n",
    "    plt.axvline(x=p25, color='red', linestyle='--', label='25th percentile')\n",
    "    plt.axvline(x=p50, color='blue', linestyle='--', label='50th percentile')\n",
    "    plt.axvline(x=p75, color='orange', linestyle='--', label='75th percentile')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c76afb-5828-4459-9ff7-f73c3539dde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "p25 = np.percentile(Y_train, 25)\n",
    "p50 = np.percentile(Y_train, 50)\n",
    "p75 = np.percentile(bh['MEDV'], 75)\n",
    "\n",
    "sns.histplot(data=Y_train, kde=True, bins=30)\n",
    "\n",
    "plt.axvline(x=p25, color='red', linestyle='--', label='25th percentile')\n",
    "plt.axvline(x=p50, color='blue', linestyle='--', label='50th percentile')\n",
    "plt.axvline(x=p75, color='orange', linestyle='--', label='75th percentile')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f35578e-57fd-4299-83aa-a08283709e78",
   "metadata": {},
   "source": [
    "We can see that RM has a lovely normal distribution, while our other feature LSTAT has quite a positive, right skewed distribution. When we take a look at our target variable, we can see a very similar pattern where MEDV is also slightly positively (right) skewed. There are [several ways](https://towardsdatascience.com/top-3-methods-for-handling-skewed-data-1334e0debf45) to tackle skewed distributions, but a very popular way to tackle right skewed distributions is to perform a **log transformation**. This is where we transform our skewed data into their logarithmic forms to create a more linear relationship between all our variables. We'll do this using numpy log function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66441e1d-95c0-49ef-872e-b1bebda43fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first lets check the skews of our features\n",
    "print('skew for RM feature:',X_train['RM'].skew())\n",
    "print('skew for LSTAT feature:',X_train['LSTAT'].skew())\n",
    "print('skew for MEDV target v:',Y_train.skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec53d14a-ebfc-4119-b608-5845f60d0a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets transform our features for our one feature LSTAT and MEDV\n",
    "LSTAT_log = np.log(X_train['LSTAT'])\n",
    "MEDV_log = np.log(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8df1f1-2f53-430b-97f6-4bf0eeb9c0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "p25 = np.percentile(MEDV_log, 25)\n",
    "p50 = np.percentile(MEDV_log, 50)\n",
    "p75 = np.percentile(MEDV_log, 75)\n",
    "\n",
    "sns.histplot(data=MEDV_log, kde=True, bins=30)\n",
    "\n",
    "plt.axvline(x=p25, color='red', linestyle='--', label='25th percentile')\n",
    "plt.axvline(x=p50, color='blue', linestyle='--', label='50th percentile')\n",
    "plt.axvline(x=p75, color='orange', linestyle='--', label='75th percentile')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8670732f-157f-481b-b30d-31c19228bdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "p25 = np.percentile(LSTAT_log, 25)\n",
    "p50 = np.percentile(LSTAT_log, 50)\n",
    "p75 = np.percentile(LSTAT_log, 75)\n",
    "\n",
    "sns.histplot(data=LSTAT_log, kde=True, bins=30)\n",
    "\n",
    "plt.axvline(x=p25, color='red', linestyle='--', label='25th percentile')\n",
    "plt.axvline(x=p50, color='blue', linestyle='--', label='50th percentile')\n",
    "plt.axvline(x=p75, color='orange', linestyle='--', label='75th percentile')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed4f228-a422-4eaa-b170-004245bba7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first lets check the skews of our features\n",
    "print('skew for LSTAT_log feature:',LSTAT_log.skew())\n",
    "print('skew for MEDV_log target v:',MEDV_log.skew())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826acde8-01ce-4e6a-9985-73b59bc1aeac",
   "metadata": {},
   "source": [
    "Visually it looks as though our MEDV variable has simply been flipped and is now left skewed instead of right skewed. oUr LSTAT feature seems to be in a similar shape. When we take a look at the numerical skew, we notice that our LSTAT variables skew got slightly better, while our MEDV skew seems to have gotten better as well. Despite being negatively skewed now, its skew has decreased from 1.1 which was very skewed to -0.34, which is a better skew. How will these variables perform on our model now? Lets check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b594a5-d70c-4260-b1c8-45fc96ae4947",
   "metadata": {},
   "outputs": [],
   "source": [
    "RM = X_train['RM']\n",
    "LSTAT_log = LSTAT_log\n",
    "\n",
    "X_train_log = pd.DataFrame({'LSTAT': LSTAT_log, 'RM': RM})\n",
    "\n",
    "Y_train_log = pd.DataFrame({'MEDV': MEDV_log})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ea2a51-5453-4cc0-91e2-2bf30bd825b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_log.shape)\n",
    "print(Y_train_log.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2aa9f4-dc7e-4fd8-a43b-74e0a73db2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define our model\n",
    "llog_model = LinearRegression()\n",
    "#Lets fit our Model to our features(X) and our target(Y) to the model\n",
    "llog_model.fit(X_train_log, Y_train_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebe3a05-ab0f-4013-b091-db91b27c8372",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting our linear model to our features to see what they give us as a prediction for y\n",
    "ylog_train_predict = llog_model.predict(X_train_log)\n",
    "#Evaluating our predicted values against the actual values to get a sense of accuracy\n",
    "logrmse = (np.sqrt(mean_squared_error(Y_train_log, ylog_train_predict)))\n",
    "#Looking to see how much of the variation in our data our model explains\n",
    "logr2 = r2_score(Y_train_log, ylog_train_predict)\n",
    "\n",
    "print(\"model performance for log training set\")\n",
    "print(\"--------------------------------------\")\n",
    "print('log RMSE is {}'.format(rmse))\n",
    "print('log R2 score is {}'.format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9552183-0955-46ed-833d-3f11f9e02b13",
   "metadata": {},
   "source": [
    "This model did a lot better than our previous training set with no log transformations. Lets see how this model now performs on our testing set. We'll also have to transorm our test values to log values as well in the same way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9461e0-5e35-4ab9-ada9-6669e627b8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTAT_log_test = np.log(X_test['LSTAT'])\n",
    "MEDV_log_test = np.log(Y_test)\n",
    "RM_test = X_test['RM']\n",
    "\n",
    "X_test_log = pd.DataFrame({'LSTAT': LSTAT_log_test, 'RM': RM_test})\n",
    "\n",
    "Y_test_log = pd.DataFrame({'MEDV': MEDV_log_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6e1a9f-1201-4f66-83ef-6def79f0a351",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same tests for the test set\n",
    "y_testlog_predict = llog_model.predict(X_test_log)\n",
    "log_testrmse = (np.sqrt(mean_squared_error(Y_test_log, y_testlog_predict)))\n",
    "log_testr2 = r2_score(Y_test_log, y_testlog_predict)\n",
    "\n",
    "print('log model evaluation for test set')\n",
    "print('-----------------------------')\n",
    "print('RMSE is {}'.format(rmse))\n",
    "print('R2 is {}'.format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cc11ae-87ee-428e-a3c1-ed58d473f22b",
   "metadata": {},
   "source": [
    "Our log transformations proved useful for our training set and gave us some increases in the R2 score meaning that our model understands our data better and a lower RMSE meaning our error rate is a little lower as well. Now lets take a look at some of our other assumptions to see if we can also make some gains in other areas. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146b8eb2-8138-4bcb-96ca-47cd03fcafc5",
   "metadata": {},
   "source": [
    "### Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c295a443-74be-447c-bc82-de9136df4736",
   "metadata": {},
   "source": [
    "Previously we tried to create a more linear relationship by trying to transform our data into more normal distributions with logarithmic transformations. Yet there are other ways to deal with non linear distributions. Another way is to fit the model on a [Polynomial regression](https://www.analyticsvidhya.com/blog/2021/07/all-you-need-to-know-about-polynomial-regression/). These types of regressions deal with variables that look to have a slight curve when plotted against the target variable instead of a straight line. We can see that our variable LSTAT has a slight curve as compared to RM, indicating that maybe a Polynomial Regression would be a good fit for it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203d4933-ca6b-4044-8059-7ea89c0153bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "\n",
    "features = ['LSTAT', 'RM']\n",
    "target = bh['MEDV']\n",
    "\n",
    "for i, col in enumerate(features):\n",
    "    plt.subplot(1, len(features), i+1)\n",
    "    x = bh[col]\n",
    "    y = target\n",
    "    plt.scatter(x,y, marker='o')\n",
    "    plt.title(col)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('MEDV')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f2e119-ce0e-4d23-9c4e-0697ce726b5f",
   "metadata": {},
   "source": [
    "Before we explore how this type of model can be used to fit our data, lets generate a random data set to understand it better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1479330f-17e5-4ae1-b3ee-6e198f25484a",
   "metadata": {},
   "source": [
    "Lets apply a linear regression model to a data set we will generate using numpy and the scikit learn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f2718e-b732-480c-8d1d-3e6b84e3c5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "#create our nonlinear data points\n",
    "np.random.seed(0)\n",
    "x = 2 - 3 * np.random.normal(0, 1, 20)\n",
    "y = x - 2 * (x ** 2) + 0.5 * (x ** 3) + np.random.normal(-3, 3, 20)\n",
    "\n",
    "#tranform our data to include another axis\n",
    "x = x[:, np.newaxis]\n",
    "y = y[:, np.newaxis]\n",
    "\n",
    "#lets fit our data onto a linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(x,y)\n",
    "y_pred = model.predict(x)\n",
    "\n",
    "#lets plot our data and also plot our line of best fit for our linear regression\n",
    "plt.scatter(x, y, s=10)\n",
    "plt.plot(x, y_pred, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bc4317-bcab-4ad1-959c-cf23ca8900af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting our linear model to our features to see what they give us as a prediction for y\n",
    "y_pred = model.predict(x)\n",
    "#Evaluating our predicted values against the actual values to get a sense of accuracy\n",
    "rmse = (np.sqrt(mean_squared_error(y, y_pred)))\n",
    "#Looking to see how much of the variation in our data our model explains\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(\"model performance\")\n",
    "print(\"------------------------------\")\n",
    "print('RMSE is {}'.format(rmse))\n",
    "print('R2 score is {}'.format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07910621-d264-47ec-9840-b2b9da3ff0b9",
   "metadata": {},
   "source": [
    "### Under-Fitting and Over-Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c61826-854c-47d0-b791-bcd4e8893106",
   "metadata": {},
   "source": [
    "We can see that our line of best fit clearly is not doing the job right and isn't able to capture the pattern of our data set at all. This is an **Under-Fitting**. Under-Fitting is something we want to circumvent when building our models and to do that we have to increase the *complexity* of our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71da70f3-8aa2-40b1-a716-208fa02f1c71",
   "metadata": {},
   "source": [
    "We'll essentially have to generate a higher order of equations. Our current equation looks like \n",
    "    Y = B0 + B1x\n",
    "But what we want instead is\"\n",
    "    Y = B0 + B1x + B2x^2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba5a18b-d8ad-4134-85d9-3aab9b11bb0a",
   "metadata": {},
   "source": [
    "This is still a linear model because the coefficients we currently have are still linear. x^2 is just an additional feature, but including it has indicated to our model that the curve we are fitting is [quadratic](https://en.wikipedia.org/wiki/Quadratic_function) in nature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e141b94-c099-4c7b-84ef-00eec055594a",
   "metadata": {},
   "source": [
    "To add and convert our current features to best fit our model, we will use the `PolynomialFeatures` class in scikit-learn. To account for our new feature of x^2 we'll have to use the [degree](https://www.investopedia.com/terms/d/degrees-of-freedom.asp#:~:text=Degrees%20of%20Freedom%20Formula&text=Some%20calculations%20of%20degrees%20of,are%20two%20parameters%20to%20estimate.) parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad53652-a561-46cb-948d-fd90d5f14665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "np.random.seed(0)\n",
    "x = 2 - 3 * np.random.normal(0, 1, 20)\n",
    "y = x - 2 * (x ** 2) + 0.5 * (x ** 3) + np.random.normal(-3, 3, 20)\n",
    "\n",
    "# transforming the data to include another axis\n",
    "x = x[:, np.newaxis]\n",
    "y = y[:, np.newaxis]\n",
    "\n",
    "#add in the total no of degrees in our equation\n",
    "polynomial_features= PolynomialFeatures(degree=2)\n",
    "x_poly = polynomial_features.fit_transform(x)\n",
    "\n",
    "#Build our model\n",
    "model = LinearRegression()\n",
    "model.fit(x_poly, y)\n",
    "y_poly_pred = model.predict(x_poly)\n",
    "\n",
    "#evaluate our metrics\n",
    "rmse = np.sqrt(mean_squared_error(y,y_poly_pred))\n",
    "r2 = r2_score(y,y_poly_pred)\n",
    "print(rmse)\n",
    "print(r2)\n",
    "\n",
    "#create our scatter plot-you have to create this after evaluating your metrics or it'll skew the data a bit\n",
    "plt.scatter(x, y, s=10)\n",
    "# sort the values of x before line plot\n",
    "sort_axis = operator.itemgetter(0)\n",
    "sorted_zip = sorted(zip(x,y_poly_pred), key=sort_axis)\n",
    "x, y_poly_pred = zip(*sorted_zip)\n",
    "plt.plot(x, y_poly_pred, color='m')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dea2cb-c272-439d-b7c3-72d7be7a9638",
   "metadata": {},
   "source": [
    "We can see that our RMSE has decreased by about 5 points meaning our error term is lower, a good sign. We can also see that our our r2 is higher as well and has increased by over 22%. This indicates that our model is getting better! Now lets try fitting a [cubic curve](https://en.wikipedia.org/wiki/Cubic_function) to our data to see how it impacts our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be13133f-4209-408c-8395-c17434124e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "x = 2 - 3 * np.random.normal(0, 1, 20)\n",
    "y = x - 2 * (x ** 2) + 0.5 * (x ** 3) + np.random.normal(-3, 3, 20)\n",
    "\n",
    "# transforming the data to include another axis\n",
    "x = x[:, np.newaxis]\n",
    "y = y[:, np.newaxis]\n",
    "\n",
    "#add in the total no of degrees in our equation\n",
    "polynomial_features= PolynomialFeatures(degree=3)\n",
    "x_poly = polynomial_features.fit_transform(x)\n",
    "\n",
    "#Build our model\n",
    "model = LinearRegression()\n",
    "model.fit(x_poly, y)\n",
    "y_poly_pred = model.predict(x_poly)\n",
    "\n",
    "#evaluate our metrics\n",
    "rmse = np.sqrt(mean_squared_error(y,y_poly_pred))\n",
    "r2 = r2_score(y,y_poly_pred)\n",
    "print(rmse)\n",
    "print(r2)\n",
    "\n",
    "#create our scatter plot-you have to create this after evaluating your metrics or it'll skew the data a bit\n",
    "plt.scatter(x, y, s=10)\n",
    "# sort the values of x before line plot\n",
    "sort_axis = operator.itemgetter(0)\n",
    "sorted_zip = sorted(zip(x,y_poly_pred), key=sort_axis)\n",
    "x, y_poly_pred = zip(*sorted_zip)\n",
    "plt.plot(x, y_poly_pred, color='m')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfc5582-37cf-444a-b2da-38939766ee20",
   "metadata": {},
   "source": [
    "We've now gotten a much lower error score, decreasing it by 7 points and our r2 has increased by 13%, and is now very close 100%. Looking at this, it looks like our cubic curve fits our data very well and has optimized our model greatly. We can go even further and increase our degrees to 20. Lets do that to see how well it fits to our data and impacts our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941a0112-57cc-4ea1-b239-d08123c7679a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "x = 2 - 3 * np.random.normal(0, 1, 20)\n",
    "y = x - 2 * (x ** 2) + 0.5 * (x ** 3) + np.random.normal(-3, 3, 20)\n",
    "\n",
    "# transforming the data to include another axis\n",
    "x = x[:, np.newaxis]\n",
    "y = y[:, np.newaxis]\n",
    "\n",
    "#add in the total no of degrees in our equation\n",
    "polynomial_features= PolynomialFeatures(degree=20)\n",
    "x_poly = polynomial_features.fit_transform(x)\n",
    "\n",
    "#Build our model\n",
    "model = LinearRegression()\n",
    "model.fit(x_poly, y)\n",
    "y_poly_pred = model.predict(x_poly)\n",
    "\n",
    "#evaluate our metrics\n",
    "rmse = np.sqrt(mean_squared_error(y,y_poly_pred))\n",
    "r2 = r2_score(y,y_poly_pred)\n",
    "print(rmse)\n",
    "print(r2)\n",
    "\n",
    "#create our scatter plot-you have to create this after evaluating your metrics or it'll skew the data a bit\n",
    "plt.scatter(x, y, s=10)\n",
    "# sort the values of x before line plot\n",
    "sort_axis = operator.itemgetter(0)\n",
    "sorted_zip = sorted(zip(x,y_poly_pred), key=sort_axis)\n",
    "x, y_poly_pred = zip(*sorted_zip)\n",
    "plt.plot(x, y_poly_pred, color='m')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4c0fca-1e8e-458a-9c76-a026705a2025",
   "metadata": {},
   "source": [
    "Our error term has decreased by 2 points, and our r2 has increased by about 1% point. Overall though it looks like this is an extremely optimal fit for our data, we don't really benefit much from having that marginal positive increase in metrics. Furthermore, this curve is also capturing a lot of the outliers and noise in our data, which is perfect for this specific set of data we have, but is extremely sub optimal for future data we decide to use this model for, future unseen data. This is an example of **over-fitting**.\n",
    "\n",
    "To correct this, we can add more training samples so the algorithm we have doesn't learn the noise in our data, but really and truly we have to learn how to choose optimal models. But how do we do that? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1540b5-0772-4396-83b1-d846ba2f0c15",
   "metadata": {},
   "source": [
    "### Bias vs Variance trade - off"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2260b39d-0bbe-4cd1-a979-56304635a0bd",
   "metadata": {},
   "source": [
    "What we have just proved with our previous work is the *bias variance trade-off*. Bias refers to the error due to our models very simple assumptions in fitting the data. A high bias means the model can't understand our data patterns. It is biased towards its basic assumptions which results in **under-fitting**. Variance refers to the error due to the complex model trying to fit the data too well. High variance means the models path is highly variable, and has the ability to fit almost every data point which results in **over-fitting**. Lets visualize all our previous models and put them side by side to really see what this looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06ea54b-6282-4895-bf39-ad44c87f7e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = [1, 3, 20]\n",
    "\n",
    "fig, axs = plt.subplots(1, len(degrees), figsize=(15, 5))\n",
    "\n",
    "titles = ['High Bias, Low Variance', 'Low Bias, Low Variance', 'Low Bias, High Variance']\n",
    "\n",
    "for i, degree in enumerate(degrees):\n",
    "    np.random.seed(0)\n",
    "    x = 2 - 3 * np.random.normal(0, 1, 20)\n",
    "    y = x - 2 * (x ** 2) + 0.5 * (x ** 3) + np.random.normal(-3, 3, 20)\n",
    "    \n",
    "    x = x[:, np.newaxis]\n",
    "    y = y[:, np.newaxis]\n",
    "    \n",
    "    polynomial_features = PolynomialFeatures(degree=degree)\n",
    "    x_poly = polynomial_features.fit_transform(x)\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(x_poly, y)\n",
    "    y_poly_pred = model.predict(x_poly)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y, y_poly_pred))\n",
    "    r2 = r2_score(y, y_poly_pred)\n",
    "    \n",
    "    axs[i].scatter(x, y, s=10)\n",
    "    sort_axis = operator.itemgetter(0)\n",
    "    sorted_zip = sorted(zip(x, y_poly_pred), key=sort_axis)\n",
    "    x, y_poly_pred = zip(*sorted_zip)\n",
    "    axs[i].plot(x, y_poly_pred, color='m')\n",
    "    \n",
    "    axs[i].set_title(titles[i]) \n",
    "    axs[i].text(0.05, 0.95, f'Degree: {degree}\\nRMSE: {rmse:.2f}\\nR2: {r2:.2f}', \n",
    "                transform=axs[i].transAxes, fontsize=12, verticalalignment='top')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feafcb45-3615-4768-9016-1a44d5bb6bab",
   "metadata": {},
   "source": [
    "We can clearly see that as our model complexity increases, its bias decreases but its variability shoots up as well. To achieve a good model that performs well both on the training set and unseen data, we'll have to make a trade-off. In this case, the optimal model would be our second model. Though its variance is higher than our first, its bias is lower than our third, and its metrics are still quite optimal. "
   ]
  },
  {
   "attachments": {
    "fcc2671c-f50e-4ad9-aba3-dbc1da96968b.webp": {
     "image/webp": "UklGRrInAABXRUJQVlA4WAoAAAAIAAAA6wEANAEAVlA4INImAABwmQCdASrsATUBPm02lkgkIyKh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\nAQAAAOwBAAADoAQAAQAAADUBAAAAAAAA\n"
    }
   },
   "cell_type": "markdown",
   "id": "1a337d8a-8763-49d6-a01d-6086542a1e96",
   "metadata": {},
   "source": [
    "![model_complexity.webp](attachment:fcc2671c-f50e-4ad9-aba3-dbc1da96968b.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f601f3fe-748c-45ee-8e5f-0de1a65874ae",
   "metadata": {},
   "source": [
    "### Applying Polynomial Regression to Our Boston Housing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e88677-1bdc-4dc8-b326-2cf28e2fd17c",
   "metadata": {},
   "source": [
    "As mentioned before, our Boston Housing data set doesn't have completely linear values. Our LSTAT value in particular has a slighly more complex pattern than a simple linear one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da6b4fb-a65e-44d3-a2cc-87ac5fd36772",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "\n",
    "features = ['LSTAT', 'RM']\n",
    "target = bh['MEDV']\n",
    "\n",
    "for i, col in enumerate(features):\n",
    "    plt.subplot(1, len(features), i+1)\n",
    "    x = bh[col]\n",
    "    y = target\n",
    "    plt.scatter(x,y, marker='o')\n",
    "    plt.title(col)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('MEDV')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311737cd-a7b3-411d-a092-12ff02cf255f",
   "metadata": {},
   "source": [
    "We've dealt with creating polynomial features with a univariate model, but how do we implement it when we have multivariate one. Lets create a function which will transform the original features into polynomial features and then apply the linear regression on it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840bdc4c-1942-46f9-bd88-f2a935f642ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def poly_model(degree):\n",
    "    poly_features = PolynomialFeatures(degree=degree)\n",
    "    \n",
    "    # transforms the existing features in our training set to higher degree features\n",
    "    X_train_poly = poly_features.fit_transform(X_train)\n",
    "    \n",
    "    #fit the transformed features to Linear Regression\n",
    "    poly_model = LinearRegression()\n",
    "    poly_model.fit(X_train_poly, Y_train)\n",
    "    \n",
    "    #lets predict now on training dataset\n",
    "    y_train_predicted = poly_model.predict(X_train_poly)\n",
    "    \n",
    "    #predicting on test dataset\n",
    "    y_test_predict = poly_model.predict(poly_features.fit_transform(X_test))\n",
    "    \n",
    "    #get evaluation metrics for training data\n",
    "    rmse_train = np.sqrt(mean_squared_error(Y_train, y_train_predict))\n",
    "    r2_train = r2_score(Y_train, y_train_predict)\n",
    "    \n",
    "    #get evaluation metrics for testing \n",
    "    rmse_test = np.sqrt(mean_squared_error(Y_test, y_test_predict))\n",
    "    r2_test = r2_score(Y_test, y_test_predict)\n",
    "    \n",
    "    print(\"The model performance for the training set\")\n",
    "    print(\"-------------------------------------------\")\n",
    "    print(\"RMSE of training set is {}\".format(rmse_train))\n",
    "    print(\"R2 score of training set is {}\".format(r2_train))\n",
    "  \n",
    "    print(\"\\n\")\n",
    "  \n",
    "    print(\"The model performance for the test set\")\n",
    "    print(\"-------------------------------------------\")\n",
    "    print(\"RMSE of test set is {}\".format(rmse_test))\n",
    "    print(\"R2 score of test set is {}\".format(r2_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abe01dd-327c-4be5-8efd-82c15e75c8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_model(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3759c9-2d2f-445d-a51d-705ff19d3164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(Y_test, y_test_predict, color = 'g')\n",
    "plt.title(\"Actual vs. Predicted\")\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predicted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe59a1f-dab1-470a-8898-0adf8ac7035a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f55f7a35-4964-4df9-b116-dba1f6c8ec92",
   "metadata": {},
   "source": [
    "https://builtin.com/machine-learning/ensemble-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada69b61-232e-476f-ae50-f83989df9698",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
